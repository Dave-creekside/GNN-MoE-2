{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u8x17zlSzp8q",
        "outputId": "6c37fe95-3eea-495d-ef73-2c9f7b08ce28"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets numpy matplotlib seaborn\n",
        "!pip uninstall -y datasets fsspec huggingface_hub transformers tokenizers\n",
        "!rm -rf ~/.cache/huggingface/datasets\n",
        "!pip install datasets==2.14.7 fsspec==2023.10.0 huggingface_hub==0.17.3 transformers==4.35.2 tokenizers==0.15.0\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXQNwA_X4FcH",
        "outputId": "8b52e9a0-356e-43b4-91d9-dcf3821ff610"
      },
      "outputs": [],
      "source": [
        "#print(torch.__version__)\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Hyperparameter Configuration</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cell 1: Hyperparameter Configuration ---\n",
        "import json\n",
        "\n",
        "class TrainingConfig:\n",
        "    def __init__(self):\n",
        "        # --- General & Model Architecture ---\n",
        "        self.run_name = \"notebook_training_run\"\n",
        "        self.embed_dim = 256\n",
        "        self.num_layers = 4\n",
        "        self.num_experts = 4\n",
        "        self.vocab_size = 50257  # GPT-2 default\n",
        "        self.max_seq_length = 1024\n",
        "        self.dropout_rate = 0.1\n",
        "\n",
        "        # --- GNN/HGNN Coupler Settings ---\n",
        "        self.coupler_type = \"HGNN\"  # \"GNN\" or \"HGNN\"\n",
        "        self.gnn_layers = 2\n",
        "        self.hgnn_conv_type = \"HypergraphConv\"\n",
        "        self.static_hyperedge_strategy = \"all_pairs\" # \"all_pairs\" or \"all_triplets\"\n",
        "        self.hgnn_learnable_edge_weights = True\n",
        "\n",
        "        # --- Training Settings ---\n",
        "        self.epochs = 5\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 3e-4\n",
        "        self.lr_scheduler_type = \"cosine\" # \"cosine\", \"linear\", \"step\"\n",
        "        self.warmup_steps = 500\n",
        "        self.weight_decay = 0.01\n",
        "        self.grad_clip_value = 1.0\n",
        "\n",
        "        # --- Dataset Settings ---\n",
        "        self.dataset_name = \"wikitext\"\n",
        "        self.dataset_config_name = \"wikitext-2-v1\"\n",
        "        self.num_train_samples = 10000\n",
        "        self.num_eval_samples = 1000\n",
        "\n",
        "        # --- Adaptive Orthogonality (Phase 2.2) ---\n",
        "        self.adaptive_weight_orthogonality = True\n",
        "        self.initial_weight_orthogonality_strength = 0.1\n",
        "        self.target_specialization_score = 0.95\n",
        "        self.adaptive_decay_schedule = \"cosine\"\n",
        "        \n",
        "        # --- Static Orthogonality (Phase 2.1) ---\n",
        "        self.apply_weight_orthogonality_loss = False # Overridden by adaptive if True\n",
        "        self.weight_orthogonality_loss_weight = 0.05\n",
        "\n",
        "        # --- Logging & Checkpointing ---\n",
        "        self.eval_every = 250 # steps\n",
        "        self.log_every = 50 # steps\n",
        "        self.checkpoint_dir = \"checkpoints_notebook\"\n",
        "\n",
        "    def to_cli_args(self):\n",
        "        \"\"\"Converts config to a list of CLI arguments for run_gnn_moe.py.\"\"\"\n",
        "        args = []\n",
        "        for key, value in self.__dict__.items():\n",
        "            if isinstance(value, bool):\n",
        "                if value:\n",
        "                    args.append(f\"--{key}\")\n",
        "            elif value is not None:\n",
        "                args.append(f\"--{key}\")\n",
        "                args.append(str(value))\n",
        "        return args\n",
        "\n",
        "# Create an instance of the config\n",
        "config = TrainingConfig()\n",
        "\n",
        "# You can modify the config here, for example:\n",
        "# config.run_name = \"my_special_experiment\"\n",
        "# config.num_experts = 8\n",
        "\n",
        "print(\"‚úÖ TrainingConfig created. Modify the 'config' object to customize your run.\")\n",
        "print(f\"‚ñ∂Ô∏è Run Name: {config.run_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Execute Training Run</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cell 2: Execute Training Run ---\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Convert the config object to CLI arguments\n",
        "cli_args = config.to_cli_args()\n",
        "\n",
        "# Construct the command\n",
        "command = [sys.executable, \"orthogon/adaptive-orthogonal/run_gnn_moe.py\"] + cli_args\n",
        "\n",
        "print(\"üöÄ Starting training run with the following command:\")\n",
        "# Print a more readable version of the command\n",
        "print(\"python orthogon/adaptive-orthogonal/run_gnn_moe.py \\\\\")\n",
        "for i in range(0, len(cli_args), 2):\n",
        "    if i + 1 < len(cli_args):\n",
        "        print(f\"  {cli_args[i]} {cli_args[i+1]} \\\\\")\n",
        "    else:\n",
        "        print(f\"  {cli_args[i]}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Execute the command\n",
        "# The output will be streamed to the notebook's output area\n",
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
        "\n",
        "while True:\n",
        "    output = process.stdout.readline()\n",
        "    if output == '' and process.poll() is not None:\n",
        "        break\n",
        "    if output:\n",
        "        print(output.strip())\n",
        "\n",
        "rc = process.poll()\n",
        "if rc == 0:\n",
        "    print(\"\\n‚úÖ Training run completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training run failed with exit code {rc}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Hyperparameter Sweep Setup</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cell 3: Hyperparameter Sweep Setup ---\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "class HyperparameterSweep:\n",
        "    def __init__(self, base_config):\n",
        "        self.base_config = base_config\n",
        "        self.sweep_params = {}\n",
        "\n",
        "    def add_sweep(self, param_name, values):\n",
        "        \"\"\"Add a parameter to sweep over.\"\"\"\n",
        "        self.sweep_params[param_name] = values\n",
        "        print(f\"Added sweep for '{param_name}' with values: {values}\")\n",
        "\n",
        "    def run_sweep(self):\n",
        "        \"\"\"Runs the hyperparameter sweep.\"\"\"\n",
        "        keys = self.sweep_params.keys()\n",
        "        values = self.sweep_params.values()\n",
        "        \n",
        "        param_combinations = list(itertools.product(*values))\n",
        "        \n",
        "        print(f\"\\nüî¨ Starting hyperparameter sweep with {len(param_combinations)} combinations.\")\n",
        "        \n",
        "        results = []\n",
        "        \n",
        "        for i, combo in enumerate(param_combinations):\n",
        "            print(f\"\\n--- Running Combination {i+1}/{len(param_combinations)} ---\")\n",
        "            \n",
        "            # Create a new config for this run\n",
        "            sweep_config = TrainingConfig()\n",
        "            sweep_config.__dict__.update(self.base_config.__dict__)\n",
        "            \n",
        "            # Apply the current combination of hyperparameters\n",
        "            for key, value in zip(keys, combo):\n",
        "                setattr(sweep_config, key, value)\n",
        "            \n",
        "            # Create a unique run name\n",
        "            run_name_parts = [f\"{key.replace('_','-')}-{value}\" for key, value in zip(keys, combo)]\n",
        "            sweep_config.run_name = f\"sweep_{'_'.join(run_name_parts)}\"\n",
        "            \n",
        "            print(f\"Run Name: {sweep_config.run_name}\")\n",
        "            \n",
        "            # Execute the training run\n",
        "            cli_args = sweep_config.to_cli_args()\n",
        "            command = [sys.executable, \"orthogon/adaptive-orthogonal/run_gnn_moe.py\"] + cli_args\n",
        "            \n",
        "            process = subprocess.run(command, capture_output=True, text=True)\n",
        "            \n",
        "            if process.returncode == 0:\n",
        "                print(\"‚úÖ Run completed successfully.\")\n",
        "                # In a real scenario, you would parse the output to get metrics\n",
        "                # For this example, we'll just record success\n",
        "                result = {key: value for key, value in zip(keys, combo)}\n",
        "                result['status'] = 'Success'\n",
        "                results.append(result)\n",
        "            else:\n",
        "                print(f\"‚ùå Run failed.\")\n",
        "                print(process.stderr)\n",
        "                result = {key: value for key, value in zip(keys, combo)}\n",
        "                result['status'] = 'Failed'\n",
        "                results.append(result)\n",
        "\n",
        "        # Display results in a table\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"\\n--- Sweep Results ---\")\n",
        "        print(results_df)\n",
        "        return results_df\n",
        "\n",
        "# --- Example Sweep ---\n",
        "# Create a sweep instance\n",
        "sweep = HyperparameterSweep(base_config=config)\n",
        "\n",
        "# Add parameters to sweep\n",
        "sweep.add_sweep('learning_rate', [1e-4, 3e-4, 5e-4])\n",
        "sweep.add_sweep('num_experts', [2, 4])\n",
        "\n",
        "# To run the sweep, uncomment the line below\n",
        "# sweep_results_df = sweep.run_sweep()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Demonstration Commands</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cell 4: Demonstration Commands ---\n",
        "\n",
        "def run_demo_command(demo_name, command_list):\n",
        "    \"\"\"Helper function to run and print a demo command.\"\"\"\n",
        "    print(f\"--- Running Demo: {demo_name} ---\")\n",
        "    \n",
        "    # Construct the command\n",
        "    command = [sys.executable, \"orthogon/adaptive-orthogonal/run_gnn_moe.py\"] + command_list\n",
        "    \n",
        "    # Print a readable version\n",
        "    print(\"python orthogon/adaptive-orthogonal/run_gnn_moe.py \\\\\")\n",
        "    for i in range(0, len(command_list), 2):\n",
        "        if i + 1 < len(command_list):\n",
        "            print(f\"  --{command_list[i]} {command_list[i+1]} \\\\\")\n",
        "        else:\n",
        "            print(f\"  --{command_list[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Execute the command\n",
        "    process = subprocess.run(command, capture_output=True, text=True)\n",
        "    \n",
        "    if process.returncode == 0:\n",
        "        print(\"‚úÖ Demo run completed successfully.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Demo run failed.\")\n",
        "        print(process.stderr)\n",
        "\n",
        "# --- Demo Configurations ---\n",
        "\n",
        "# 1. Standard GNN-MoE\n",
        "demo_gnn_moe = {\n",
        "    \"run_name\": \"demo_gnn_moe\",\n",
        "    \"coupler_type\": \"GNN\",\n",
        "    \"num_experts\": 4,\n",
        "    \"epochs\": 1,\n",
        "    \"max_batches_per_epoch\": 100 # For a quick run\n",
        "}\n",
        "\n",
        "# 2. HGNN-MoE (Static Orthogonality)\n",
        "demo_hgnn_moe = {\n",
        "    \"run_name\": \"demo_hgnn_moe_static_ortho\",\n",
        "    \"coupler_type\": \"HGNN\",\n",
        "    \"num_experts\": 4,\n",
        "    \"apply_weight_orthogonality_loss\": True,\n",
        "    \"weight_orthogonality_loss_weight\": 0.05,\n",
        "    \"epochs\": 1,\n",
        "    \"max_batches_per_epoch\": 100\n",
        "}\n",
        "\n",
        "# 3. HGNN-MoE (Adaptive Orthogonality)\n",
        "demo_adaptive_hgnn_moe = {\n",
        "    \"run_name\": \"demo_hgnn_moe_adaptive_ortho\",\n",
        "    \"coupler_type\": \"HGNN\",\n",
        "    \"num_experts\": 4,\n",
        "    \"adaptive_weight_orthogonality\": True,\n",
        "    \"initial_weight_orthogonality_strength\": 0.1,\n",
        "    \"epochs\": 1,\n",
        "    \"max_batches_per_epoch\": 100\n",
        "}\n",
        "\n",
        "# --- To run a demo, uncomment one of the lines below ---\n",
        "# run_demo_command(\"GNN-MoE\", [f\"--{k}\" if isinstance(v, bool) and v else f\"--{k} {v}\" for k, v in demo_gnn_moe.items() for item in ((k,v),) if not (isinstance(v, bool) and not v)])\n",
        "# run_demo_command(\"HGNN-MoE (Static Ortho)\", [f\"--{k}\" if isinstance(v, bool) and v else f\"--{k} {v}\" for k, v in demo_hgnn_moe.items() for item in ((k,v),) if not (isinstance(v, bool) and not v)])\n",
        "# run_demo_command(\"HGNN-MoE (Adaptive Ortho)\", [f\"--{k}\" if isinstance(v, bool) and v else f\"--{k} {v}\" for k, v in demo_adaptive_hgnn_moe.items() for item in ((k,v),) if not (isinstance(v, bool) and not v)])\n",
        "\n",
        "# A better way to run the demos\n",
        "def dict_to_cli_args(d):\n",
        "    args = []\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, bool):\n",
        "            if value:\n",
        "                args.append(f\"--{key}\")\n",
        "        elif value is not None:\n",
        "            args.append(f\"--{key}\")\n",
        "            args.append(str(value))\n",
        "    return args\n",
        "\n",
        "# To run a demo, uncomment one of the lines below\n",
        "# run_demo_command(\"GNN-MoE\", dict_to_cli_args(demo_gnn_moe))\n",
        "# run_demo_command(\"HGNN-MoE (Static Ortho)\", dict_to_cli_args(demo_hgnn_moe))\n",
        "# run_demo_command(\"HGNN-MoE (Adaptive Ortho)\", dict_to_cli_args(demo_adaptive_hgnn_moe))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Architectural Defaults</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ---\n",
        "# ## ‚öôÔ∏è Command-Line Argument Reference\n",
        "\n",
        "This section provides a comprehensive reference for all the command-line arguments available in the `run_gnn_moe.py` script. These arguments correspond to the fields in the `GNNMoEConfig` class.\n",
        "\n",
        "### **Model Architecture**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--embed_dim` | int | 512 | The dimensionality of the token embeddings. |\n",
        "| `--num_layers` | int | 8 | The number of transformer layers in the model. |\n",
        "| `--num_heads` | int | 8 | The number of attention heads in each transformer layer. |\n",
        "| `--num_experts` | int | 4 | The number of experts in each Mixture of Experts (MoE) layer. |\n",
        "| `--ffn_dim_multiplier` | int | 4 | Multiplier for the feed-forward network dimension relative to `embed_dim`. |\n",
        "| `--vocab_size` | int | 50257 | The size of the vocabulary (defaults to GPT-2's vocab size). |\n",
        "| `--max_seq_length` | int | 1024 | The maximum sequence length the model can handle. |\n",
        "| `--dropout_rate` | float | 0.1 | The dropout rate used in the model. |\n",
        "\n",
        "### **GNN / HGNN Coupler**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--coupler_type` | str | \"GNN\" | The type of expert coupler to use. Options: `\"GNN\"`, `\"HGNN\"`. |\n",
        "| `--gnn_layers` | int | 1 | The number of layers in the GNN/HGNN coupler. |\n",
        "| `--hgnn_conv_type` | str | \"HypergraphConv\" | The type of PyG convolution to use for the HGNN. |\n",
        "| `--static_hyperedge_strategy` | str | \"all_pairs\" | The strategy for creating static hyperedges. Options: `\"all_pairs\"`, `\"all_triplets\"`. |\n",
        "| `--hgnn_learnable_edge_weights`| bool | `False` | If set, the weights of the hyperedges will be learnable parameters. |\n",
        "\n",
        "### **Training Parameters**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--epochs` | int | 5 | The total number of training epochs. |\n",
        "| `--batch_size` | int | 32 | The number of sequences in each training batch. |\n",
        "| `--learning_rate` | float | 3e-4 | The initial learning rate for the AdamW optimizer. |\n",
        "| `--lr_scheduler_type` | str | \"cosine\" | The learning rate scheduler type. Options: `\"cosine\"`, `\"linear\"`, `\"step\"`. |\n",
        "| `--warmup_steps` | int | 1000 | The number of warmup steps for the learning rate scheduler. |\n",
        "| `--weight_decay` | float | 0.01 | The weight decay to apply during optimization. |\n",
        "| `--grad_clip_value` | float | 1.0 | The value to clip gradients at to prevent exploding gradients. |\n",
        "\n",
        "### **Dataset & Data Handling**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--dataset_name` | str | \"wikitext\" | The name of the Hugging Face dataset to use. |\n",
        "| `--dataset_config_name` | str | \"wikitext-2-v1\"| The specific configuration of the dataset. |\n",
        "| `--num_train_samples` | int | `None` | The number of training samples to use (if `None`, uses the full dataset). |\n",
        "| `--num_eval_samples` | int | `None` | The number of evaluation samples to use (if `None`, uses the full dataset). |\n",
        "| `--num_test_samples` | int | `None` | The number of test samples to use (if `None`, uses the full dataset). |\n",
        "\n",
        "### **Static Weight Orthogonality (Phase 2.1)**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--apply_weight_orthogonality_loss` | bool | `False` | If set, applies a static orthogonality loss to the expert weights. |\n",
        "| `--weight_orthogonality_loss_weight` | float | 0.01 | The strength of the static orthogonality loss. |\n",
        "| `--weight_orthogonality_normalization` | str | \"frobenius\" | The normalization method for the orthogonality loss. |\n",
        "\n",
        "### **Adaptive Weight Orthogonality (Phase 2.2)**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--adaptive_weight_orthogonality` | bool | `False` | If set, enables the adaptive orthogonality system. This overrides static settings. |\n",
        "| `--initial_weight_orthogonality_strength`| float | 0.1 | The starting strength of the adaptive orthogonality constraint. |\n",
        "| `--minimum_weight_orthogonality_strength`| float | 0.001 | The minimum strength the constraint can decay to. |\n",
        "| `--maximum_weight_orthogonality_strength`| float | 0.3 | The maximum strength the constraint can be boosted to. |\n",
        "| `--adaptive_decay_schedule` | str | \"cosine\" | The decay schedule for the constraint strength. Options: `\"cosine\"`, `\"exponential\"`, `\"linear\"`, `\"step\"`. |\n",
        "| `--adaptation_frequency` | int | 500 | The number of training steps between adaptation checks. |\n",
        "| `--target_specialization_score` | float | 0.95 | The target orthogonality score the controller aims for. |\n",
        "| `--specialization_tolerance` | float | 0.02 | The tolerance band (¬±) around the target specialization score. |\n",
        "| `--layer_specific_adaptation` | bool | `True` | If set, applies different constraint strengths to different layers. |\n",
        "| `--deeper_layer_scaling` | float | 0.8 | The scaling factor for reducing constraint strength in deeper layers. |\n",
        "| `--performance_aware_adaptation` | bool | `True` | If set, the controller considers model performance when adapting. |\n",
        "| `--performance_monitor_window` | int | 100 | The number of steps to average performance over. |\n",
        "| `--collapse_detection_threshold` | float | 0.1 | The specialization score threshold for detecting expert collapse. |\n",
        "| `--emergency_constraint_boost` | bool | `True` | If set, enables the emergency boost mechanism to prevent collapse. |\n",
        "| `--emergency_boost_multiplier` | float | 2.0 | The multiplier for the constraint strength during an emergency boost. |\n",
        "\n",
        "### **Logging & System**\n",
        "\n",
        "| Argument | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `--run_name` | str | `None` | A unique name for the training run. If `None`, a name is generated. |\n",
        "| `--checkpoint_dir` | str | \"./checkpoints\" | The directory to save model checkpoints. |\n",
        "| `--log_every` | int | 100 | The number of steps between logging training progress. |\n",
        "| `--eval_every` | int | 500 | The number of steps between running evaluation. |\n",
        "| `--save_best_only` | bool | `True` | If set, only saves the checkpoint with the best evaluation loss. |\n",
        "| `--seed` | int | 42 | The random seed for reproducibility. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
