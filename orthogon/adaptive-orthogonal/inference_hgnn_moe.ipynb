{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u8x17zlSzp8q",
        "outputId": "6c37fe95-3eea-495d-ef73-2c9f7b08ce28"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets numpy matplotlib seaborn\n",
        "!pip uninstall -y datasets fsspec huggingface_hub transformers tokenizers\n",
        "!rm -rf ~/.cache/huggingface/datasets\n",
        "!pip install datasets==2.14.7 fsspec==2023.10.0 huggingface_hub==0.17.3 transformers==4.35.2 tokenizers==0.15.0\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_493nvLi32n"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXQNwA_X4FcH",
        "outputId": "8b52e9a0-356e-43b4-91d9-dcf3821ff610"
      },
      "outputs": [],
      "source": [
        "#print(torch.__version__)\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuvohLe0Mqd8"
      },
      "source": [
        "<h1>Imports and Helper Functions</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGvQC2DYchRt",
        "outputId": "306e516f-1f53-4567-c6c0-64fb9a76ccac"
      },
      "outputs": [],
      "source": [
        "# --- Cell 1: Imports and Helper Functions ---\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Assuming the following files are in the same directory as the notebook:\n",
        "# gnn_moe_config.py\n",
        "# gnn_moe_architecture.py\n",
        "\n",
        "from gnn_moe_config import GNNMoEConfig\n",
        "from gnn_moe_architecture import GNNMoEModel\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"ðŸŽ² Random seed set to: {seed}\")\n",
        "\n",
        "def detect_device() -> torch.device:\n",
        "    \"\"\"Automatically detect the best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"ðŸš€ Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"ðŸš€ Using Apple MPS\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"ðŸš€ Using CPU\")\n",
        "    return device\n",
        "\n",
        "def load_config_from_json(config_path: str) -> GNNMoEConfig:\n",
        "    \"\"\"Load GNNMoEConfig from a JSON file.\"\"\"\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
        "    \n",
        "    print(f\"ðŸ“‹ Loading config from {config_path}\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "    \n",
        "    config = GNNMoEConfig()\n",
        "    for key, value in config_dict.items():\n",
        "        if hasattr(config, key):\n",
        "            setattr(config, key, value)\n",
        "    \n",
        "    config.__post_init__()\n",
        "    print(f\"âœ… Config loaded successfully.\")\n",
        "    return config\n",
        "\n",
        "def load_model_and_checkpoint(config: GNNMoEConfig, checkpoint_path: str, device: torch.device) -> GNNMoEModel:\n",
        "    \"\"\"Load the model and its trained weights from checkpoint.\"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "    \n",
        "    print(f\"ðŸ§  Creating GNNMoEModel...\")\n",
        "    model = GNNMoEModel(config)\n",
        "    \n",
        "    print(f\"ðŸ”„ Loading weights from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    \n",
        "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
        "    model.load_state_dict(state_dict)\n",
        "    \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(\"âœ… Model loaded and in evaluation mode.\")\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(tokenizer_name: Optional[str] = 'gpt2') -> AutoTokenizer:\n",
        "    \"\"\"Load the tokenizer.\"\"\"\n",
        "    print(f\"ðŸ”¤ Loading tokenizer: {tokenizer_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer\n",
        "\n",
        "def apply_sampling(logits: torch.Tensor, temperature: float = 1.0, top_k: int = 0, top_p: float = 0.0) -> torch.Tensor:\n",
        "    \"\"\"Apply temperature, top-k, and top-p sampling to logits.\"\"\"\n",
        "    if temperature != 1.0:\n",
        "        logits = logits / temperature\n",
        "    \n",
        "    if top_k > 0:\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "        logits_filtered = torch.full_like(logits, float('-inf'))\n",
        "        logits_filtered.scatter_(-1, top_k_indices, top_k_logits)\n",
        "        logits = logits_filtered\n",
        "    \n",
        "    if top_p > 0.0 and top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = float('-inf')\n",
        "    \n",
        "    return logits\n",
        "\n",
        "def generate_text(\n",
        "    model: GNNMoEModel,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    device: torch.device,\n",
        "    max_new_tokens: int = 100,\n",
        "    temperature: float = 0.7,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9\n",
        ") -> str:\n",
        "    \"\"\"Generate text using autoregressive decoding.\"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    generated_ids = input_ids\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            if generated_ids.shape[1] >= model.config.max_seq_length:\n",
        "                break\n",
        "            \n",
        "            outputs = model(generated_ids, attention_mask=attention_mask)\n",
        "            next_token_logits = outputs['logits'][:, -1, :]\n",
        "            filtered_logits = apply_sampling(next_token_logits, temperature, top_k, top_p)\n",
        "            \n",
        "            next_token_id = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1)\n",
        "            \n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "                \n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"âœ… All imports and helper functions are defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Configuration and Model Loading</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2AMEeBWd9LL"
      },
      "outputs": [],
      "source": [
        "# --- Cell 2: Configuration and Model Loading ---\n",
        "\n",
        "# --- Parameters to set ---\n",
        "CHECKPOINT_PATH = \"checkpoints/your_model_checkpoint.pth.tar\"  # <-- â— UPDATE THIS PATH\n",
        "CONFIG_PATH = \"checkpoints/your_config.json\"              # <-- â— UPDATE THIS PATH\n",
        "SEED = 42\n",
        "# -------------------------\n",
        "\n",
        "set_seed(SEED)\n",
        "device = detect_device()\n",
        "\n",
        "try:\n",
        "    # Load configuration and model\n",
        "    config = load_config_from_json(CONFIG_PATH)\n",
        "    model = load_model_and_checkpoint(config, CHECKPOINT_PATH, device)\n",
        "    tokenizer = load_tokenizer()\n",
        "\n",
        "    # Display model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nðŸ“Š Model Summary:\")\n",
        "    print(f\"   - Experts: {config.num_experts}\")\n",
        "    print(f\"   - Embedding Dim: {config.embed_dim}\")\n",
        "    print(f\"   - Model Layers: {config.num_layers}\")\n",
        "    print(f\"   - Coupler Type: {getattr(config, 'coupler_type', 'GNN')}\")\n",
        "    print(f\"   - Total Parameters: {total_params:,}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ {e}\")\n",
        "    print(\"   Please update the CHECKPOINT_PATH and CONFIG_PATH variables in this cell.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Text Generation</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- Cell 3: Text Generation ---\n",
        "\n",
        "# --- Generation Parameters ---\n",
        "prompt_text = \"The field of artificial intelligence is\" # <-- âœï¸ Your prompt here\n",
        "max_new_tokens = 150\n",
        "temperature = 0.75\n",
        "top_k = 50\n",
        "top_p = 0.95\n",
        "# ---------------------------\n",
        "\n",
        "if 'model' in locals() and 'tokenizer' in locals():\n",
        "    print(f\"ðŸ’¬ Generating text from prompt: '{prompt_text}'\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    generated_output = generate_text(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt_text,\n",
        "        device=device,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p\n",
        "    )\n",
        "    \n",
        "    print(\"\\nâœ¨ Generated Text âœ¨\")\n",
        "    print(\"-\" * 50)\n",
        "    print(generated_output)\n",
        "else:\n",
        "    print(\"Model not loaded. Please run Cell 2 successfully first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
