{
  "dataset_source": "local_file",
  "dataset_name": "data/test_qa_dataset.json",
  "dataset_config_name": null,
  "tokenizer": "gpt2",
  "max_seq_length": 128,
  "vocab_size": 50257,
  "num_train_samples": 1,
  "num_eval_samples": 1,
  "train_shape": [
    1,
    128
  ],
  "eval_shape": [
    1,
    128
  ]
}